{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\ipykernel\\parentpoller.py:113: UserWarning: Parent poll failed.  If the frontend dies,\n",
      "                the kernel may be left running.  Please let us know\n",
      "                about your system (bitness, Python, etc.) at\n",
      "                ipython-dev@scipy.org\n",
      "  warnings.warn(\"\"\"Parent poll failed.  If the frontend dies,\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPU\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only use the first GPU\n",
    "  try:\n",
    "    tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "  except RuntimeError as e:\n",
    "    # Visible devices must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img\n",
    "from tensorflow.keras import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Your data In"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48, 48, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_example = cv2.imread(r'D:\\Satyajit\\Code\\Projects\\Developing\\Facial Expressions Classifier\\face-expression-recognition-dataset\\fer_directory\\Training\\Happy\\7.jpg')\n",
    "img_example\n",
    "print(img_example.shape)\n",
    "plt.imshow(img_example);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_path = r'D:\\Satyajit\\Code\\Projects\\Developing\\Facial Expressions Classifier\\face-expression-recognition-dataset\\fer_directory\\Training'\n",
    "valid_images_path = r'D:\\Satyajit\\Code\\Projects\\Developing\\Facial Expressions Classifier\\face-expression-recognition-dataset\\fer_directory\\Validation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = r'D:\\Satyajit\\Code\\Projects\\Developing\\Facial Expressions Classifier\\face-expression-recognition-dataset\\fer2013\\fer2013.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Via Keras's ImageDataGeneratorClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = ImageDataGenerator(rescale=1./255)\n",
    "valid_gen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_gen = ImageDataGenerator(rescale=1./255, zoom_range=0.2, horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zoom_range=0.2, horizontal_flip=True, width_shift_range=0.2, height_shift_range=0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ImageDataGenerator Method 1:\n",
    "`Flow From Directory`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "train_flow = train_gen.flow_from_directory(train_images_path, color_mode='grayscale', target_size=(48, 48))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3589 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "valid_flow = valid_gen.flow_from_directory(valid_images_path, color_mode='grayscale', target_size=(48, 48))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ImageDataGenerator Method 2:\n",
    "`Flow (From Array)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['emotion', 'pixels', 'Usage'], dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Training', 'PublicTest', 'PrivateTest'], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Usage.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pixels'] = df.pixels.apply(lambda x: np.fromstring(x, dtype=int, sep=' ').reshape((48,48,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=48x48 at 0x1ED1B9AD220>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 7\n",
    "print(df['emotion'][i])\n",
    "array_to_img(df['pixels'][i].reshape((48,48,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_array(s):\n",
    "    arr = np.array(list(s)).reshape((-1,48,48,1))\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cw = df[df.Usage=='Training'].emotion.value_counts()\n",
    "class_weight_dict = dict(cw/sum(cw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_split(data):\n",
    "    train = data[data.Usage=='Training']\n",
    "    val = data[data.Usage=='PublicTest']\n",
    "    test = data[data.Usage=='PrivateTest']\n",
    "    x_train, y_train = train['pixels'], train['emotion']\n",
    "    x_val, y_val = val['pixels'], val['emotion']\n",
    "    x_test, y_test = test['pixels'], test['emotion']\n",
    "    return convert_to_array(x_train), convert_to_array(x_val), convert_to_array(x_test), utils.to_categorical(y_train, num_classes=7),  utils.to_categorical(y_val, num_classes=7),  utils.to_categorical(y_test, num_classes=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, x_text, y_train, y_val, y_test = train_val_test_split(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_gen.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Aug_flow = aug_gen.flow(x_train, y_train, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_flow = valid_gen.flow(x_val, y_val, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Via OpenCV2 & OS module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, valid_x, train_y, valid_y = [], [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [category for category in os.listdir(train_images_path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in os.listdir(train_images_path):\n",
    "    cat_path = os.path.join(train_images_path, category)\n",
    "    for image in os.listdir(cat_path):\n",
    "        img_path = os.path.join(cat_path, image)\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        img_array = cv2.resize(img, (48,48))\n",
    "        train_x.append(img_array)\n",
    "        train_y.append(classes.index(category))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = utils.to_categorical(np.array(train_y), num_classes=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.array(train_x).reshape(-1, 48, 48, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28709, 48, 48, 1)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28709, 7)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in os.listdir(valid_images_path):\n",
    "    cat_path = os.path.join(valid_images_path, category)\n",
    "    for image in os.listdir(cat_path):\n",
    "        img_path = os.path.join(cat_path, image)\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        img_array = cv2.resize(img, (48,48))\n",
    "        valid_x.append(img_array)\n",
    "        valid_y.append(classes.index(category))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_y = utils.to_categorical(np.array(valid_y), num_classes=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_x = np.array(valid_x).reshape(-1, 48, 48, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization,AveragePooling2D\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "emo_model = Sequential()\n",
    "\n",
    "emo_model.add(Conv2D(input_shape=train_flow.image_shape, filters=64, kernel_size=(3,3), activation='relu', data_format='channels_last', kernel_regularizer=l2(0.01)))\n",
    "emo_model.add(Conv2D(filters=64, kernel_size=(4,4), activation='relu', data_format='channels_last', padding='same'))\n",
    "emo_model.add(BatchNormalization())\n",
    "emo_model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "emo_model.add(Dropout(0.5))\n",
    "\n",
    "emo_model.add(Conv2D(filters=128, kernel_size=(4,4), activation='relu', data_format='channels_last', padding='same'))\n",
    "emo_model.add(BatchNormalization())\n",
    "emo_model.add(Conv2D(filters=128, kernel_size=(4,4), activation='relu', data_format='channels_last', padding='same'))\n",
    "emo_model.add(BatchNormalization())\n",
    "emo_model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "emo_model.add(Dropout(0.5))\n",
    "\n",
    "emo_model.add(Conv2D(filters=256, kernel_size=(4,4), activation='relu', data_format='channels_last', padding='same'))\n",
    "emo_model.add(BatchNormalization())\n",
    "emo_model.add(Conv2D(filters=256, kernel_size=(4,4), activation='relu', data_format='channels_last', padding='same'))\n",
    "emo_model.add(BatchNormalization())\n",
    "emo_model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "emo_model.add(Dropout(0.5))\n",
    "\n",
    "emo_model.add(Flatten())\n",
    "\n",
    "# emo_model.add(Dense(1024, activation='relu'))\n",
    "# emo_model.add(Dropout(0.2))\n",
    "\n",
    "emo_model.add(Dense(train_flow.num_classes, activation=\"softmax\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "emo_model.compile(optimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-7), loss=categorical_crossentropy, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": " Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[node sequential/conv2d/Conv2D (defined at <ipython-input-40-e0f4fd78498b>:1) ]] [Op:__inference_train_function_2083]\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-e0f4fd78498b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m emo_model.fit(Aug_flow, \n\u001b[0m\u001b[0;32m      2\u001b[0m               \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#               batch_size=BATCH_SIZE,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mAug_flow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m//\u001b[0m\u001b[0mAug_flow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m               \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mval_flow\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    838\u001b[0m         \u001b[1;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    839\u001b[0m         \u001b[1;31m# stateless function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 840\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    841\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    842\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1842\u001b[0m     \"\"\"\n\u001b[1;32m-> 1843\u001b[1;33m     return self._call_flat(\n\u001b[0m\u001b[0;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[0;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1923\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnknownError\u001b[0m:  Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[node sequential/conv2d/Conv2D (defined at <ipython-input-40-e0f4fd78498b>:1) ]] [Op:__inference_train_function_2083]\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "emo_model.fit(Aug_flow, \n",
    "              epochs=EPOCHS, \n",
    "#               batch_size=BATCH_SIZE,\n",
    "              steps_per_epoch=Aug_flow.n//Aug_flow.batch_size,\n",
    "              validation_data = val_flow,\n",
    "              validation_steps = val_flow.n//val_flow.batch_size,\n",
    "              callbacks=[ReduceLROnPlateau(), EarlyStopping(patience=3)],\n",
    "              shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "897/897 [==============================] - 27s 30ms/step - loss: 2.2749 - accuracy: 0.2910 - val_loss: 1.6618 - val_accuracy: 0.3680\n",
      "Epoch 2/30\n",
      "897/897 [==============================] - 26s 29ms/step - loss: 1.5370 - accuracy: 0.4175 - val_loss: 1.7250 - val_accuracy: 0.3730\n",
      "Epoch 3/30\n",
      "897/897 [==============================] - 26s 29ms/step - loss: 1.3440 - accuracy: 0.4892 - val_loss: 1.4398 - val_accuracy: 0.4587\n",
      "Epoch 4/30\n",
      "897/897 [==============================] - 27s 30ms/step - loss: 1.2520 - accuracy: 0.5298 - val_loss: 1.3092 - val_accuracy: 0.4880\n",
      "Epoch 5/30\n",
      "897/897 [==============================] - 27s 30ms/step - loss: 1.1886 - accuracy: 0.5565 - val_loss: 1.3473 - val_accuracy: 0.4989\n",
      "Epoch 6/30\n",
      "897/897 [==============================] - 27s 30ms/step - loss: 1.1323 - accuracy: 0.5781 - val_loss: 1.1452 - val_accuracy: 0.5762\n",
      "Epoch 7/30\n",
      "897/897 [==============================] - 27s 30ms/step - loss: 1.0910 - accuracy: 0.5914 - val_loss: 1.1267 - val_accuracy: 0.5873\n",
      "Epoch 8/30\n",
      "897/897 [==============================] - 27s 30ms/step - loss: 1.0520 - accuracy: 0.6069 - val_loss: 1.1068 - val_accuracy: 0.5924\n",
      "Epoch 9/30\n",
      "897/897 [==============================] - 28s 31ms/step - loss: 1.0106 - accuracy: 0.6240 - val_loss: 1.0474 - val_accuracy: 0.6144\n",
      "Epoch 10/30\n",
      "897/897 [==============================] - 28s 32ms/step - loss: 0.9721 - accuracy: 0.6389 - val_loss: 1.0737 - val_accuracy: 0.6122\n",
      "Epoch 11/30\n",
      "897/897 [==============================] - 28s 32ms/step - loss: 0.9422 - accuracy: 0.6519 - val_loss: 1.0403 - val_accuracy: 0.6261\n",
      "Epoch 12/30\n",
      "897/897 [==============================] - 29s 33ms/step - loss: 0.9043 - accuracy: 0.6688 - val_loss: 1.0731 - val_accuracy: 0.6041\n",
      "Epoch 13/30\n",
      "897/897 [==============================] - 29s 32ms/step - loss: 0.8691 - accuracy: 0.6803 - val_loss: 1.0375 - val_accuracy: 0.6275\n",
      "Epoch 14/30\n",
      "897/897 [==============================] - 29s 32ms/step - loss: 0.8357 - accuracy: 0.6917 - val_loss: 1.0467 - val_accuracy: 0.6200\n",
      "Epoch 15/30\n",
      "897/897 [==============================] - 29s 33ms/step - loss: 0.8132 - accuracy: 0.7040 - val_loss: 1.0398 - val_accuracy: 0.6177\n",
      "Epoch 16/30\n",
      "897/897 [==============================] - 29s 33ms/step - loss: 0.7818 - accuracy: 0.7154 - val_loss: 1.0292 - val_accuracy: 0.6258\n",
      "Epoch 17/30\n",
      "897/897 [==============================] - 28s 31ms/step - loss: 0.7543 - accuracy: 0.7227 - val_loss: 1.0410 - val_accuracy: 0.6281\n",
      "Epoch 18/30\n",
      "897/897 [==============================] - 29s 32ms/step - loss: 0.7814 - accuracy: 0.7161 - val_loss: 1.0627 - val_accuracy: 0.6306\n",
      "Epoch 19/30\n",
      "897/897 [==============================] - 28s 32ms/step - loss: 0.7091 - accuracy: 0.7421 - val_loss: 1.0330 - val_accuracy: 0.6378\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1d40f77f2b0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emo_model.fit(train_flow, \n",
    "              epochs=30, \n",
    "              steps_per_epoch=train_flow.n//train_flow.batch_size, \n",
    "              validation_data = valid_flow, \n",
    "              validation_steps = valid_flow.n//valid_flow.batch_size,\n",
    "              callbacks=[ReduceLROnPlateau(), EarlyStopping(patience=3)]\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "897/897 [==============================] - 28s 31ms/step - loss: 0.4519 - accuracy: 0.2686 - val_loss: 9.6063 - val_accuracy: 0.1362\n",
      "Epoch 2/100\n",
      "897/897 [==============================] - 27s 31ms/step - loss: 0.2727 - accuracy: 0.3688 - val_loss: 1.5880 - val_accuracy: 0.3669\n",
      "Epoch 3/100\n",
      "897/897 [==============================] - 28s 31ms/step - loss: 0.2230 - accuracy: 0.4397 - val_loss: 1.6243 - val_accuracy: 0.4224\n",
      "Epoch 4/100\n",
      "897/897 [==============================] - 28s 31ms/step - loss: 0.1993 - accuracy: 0.4918 - val_loss: 1.4665 - val_accuracy: 0.4584\n",
      "Epoch 5/100\n",
      "897/897 [==============================] - 28s 31ms/step - loss: 0.1852 - accuracy: 0.5256 - val_loss: 1.4319 - val_accuracy: 0.4980\n",
      "Epoch 6/100\n",
      "897/897 [==============================] - 29s 33ms/step - loss: 0.1752 - accuracy: 0.5472 - val_loss: 1.1918 - val_accuracy: 0.5550\n",
      "Epoch 7/100\n",
      "897/897 [==============================] - 29s 32ms/step - loss: 0.1659 - accuracy: 0.5697 - val_loss: 1.3977 - val_accuracy: 0.4754\n",
      "Epoch 8/100\n",
      "897/897 [==============================] - 29s 32ms/step - loss: 0.1598 - accuracy: 0.5874 - val_loss: 1.1163 - val_accuracy: 0.5848\n",
      "Epoch 9/100\n",
      "897/897 [==============================] - 28s 32ms/step - loss: 0.1543 - accuracy: 0.6019 - val_loss: 1.2053 - val_accuracy: 0.5569\n",
      "Epoch 10/100\n",
      "897/897 [==============================] - 29s 32ms/step - loss: 0.1480 - accuracy: 0.6204 - val_loss: 1.1361 - val_accuracy: 0.5926\n",
      "Epoch 11/100\n",
      "897/897 [==============================] - 29s 32ms/step - loss: 0.1419 - accuracy: 0.6347 - val_loss: 1.1847 - val_accuracy: 0.5689\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x24dc73f6a30>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emo_model.fit(train_flow, \n",
    "              epochs=100, \n",
    "              steps_per_epoch=train_flow.n//train_flow.batch_size, \n",
    "              validation_data = valid_flow, \n",
    "              validation_steps = valid_flow.n//valid_flow.batch_size,\n",
    "              callbacks=[ReduceLROnPlateau(), EarlyStopping(patience=3)],\n",
    "              class_weight = class_weight_dict\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
