{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPU\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only use the first GPU\n",
    "  try:\n",
    "    tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "  except RuntimeError as e:\n",
    "    # Visible devices must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img\n",
    "from tensorflow.keras import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Your data In"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48, 48, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_example = cv2.imread(r'D:\\Satyajit\\Code\\Projects\\Developing\\Facial Expressions Classifier\\face-expression-recognition-dataset\\fer_directory\\Training\\Happy\\7.jpg')\n",
    "img_example\n",
    "print(img_example.shape)\n",
    "plt.imshow(img_example);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_path = r'D:\\Satyajit\\Code\\Projects\\Developing\\Facial Expressions Classifier\\face-expression-recognition-dataset\\fer_directory\\Training'\n",
    "valid_images_path = r'D:\\Satyajit\\Code\\Projects\\Developing\\Facial Expressions Classifier\\face-expression-recognition-dataset\\fer_directory\\Validation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = r'D:\\Satyajit\\Code\\Projects\\Developing\\Facial Expressions Classifier\\face-expression-recognition-dataset\\fer2013\\fer2013.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Via Keras's ImageDataGeneratorClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = ImageDataGenerator(rescale=1./255)\n",
    "valid_gen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_gen = ImageDataGenerator(rescale=1./255, zoom_range=0.2, horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zoom_range=0.2, horizontal_flip=True, width_shift_range=0.2, height_shift_range=0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ImageDataGenerator Method 1:\n",
    "`Flow From Directory`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_flow = train_gen.flow_from_directory(train_images_path, color_mode='grayscale', target_size=(48, 48))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "valid_flow = valid_gen.flow_from_directory(valid_images_path, color_mode='grayscale', target_size=(48, 48))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ImageDataGenerator Method 2:\n",
    "`Flow (From Array)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Usage.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pixels'] = df.pixels.apply(lambda x: np.fromstring(x, dtype=int, sep=' ').reshape((48,48,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 7\n",
    "print(df['emotion'][i])\n",
    "array_to_img(df['pixels'][i].reshape((48,48,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_array(s):\n",
    "    arr = np.array(list(s)).reshape((-1,48,48,1))\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cw = df[df.Usage=='Training'].emotion.value_counts()\n",
    "class_weight_dict = dict(cw/sum(cw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_split(data):\n",
    "    train = data[data.Usage=='Training']\n",
    "    val = data[data.Usage=='PublicTest']\n",
    "    test = data[data.Usage=='PrivateTest']\n",
    "    x_train, y_train = train['pixels'], train['emotion']\n",
    "    x_val, y_val = val['pixels'], val['emotion']\n",
    "    x_test, y_test = test['pixels'], test['emotion']\n",
    "    return convert_to_array(x_train), convert_to_array(x_val), convert_to_array(x_test), utils.to_categorical(y_train, num_classes=7),  utils.to_categorical(y_val, num_classes=7),  utils.to_categorical(y_test, num_classes=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, x_text, y_train, y_val, y_test = train_val_test_split(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_gen.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Aug_flow = aug_gen.flow(x_train, y_train, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_flow = valid_gen.flow(x_val, y_val, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Via OpenCV2 & OS module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, valid_x, train_y, valid_y = [], [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [category for category in os.listdir(train_images_path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in os.listdir(train_images_path):\n",
    "    cat_path = os.path.join(train_images_path, category)\n",
    "    for image in os.listdir(cat_path):\n",
    "        img_path = os.path.join(cat_path, image)\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        img_array = cv2.resize(img, (48,48))\n",
    "        train_x.append(img_array)\n",
    "        train_y.append(classes.index(category))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = utils.to_categorical(np.array(train_y), num_classes=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.array(train_x).reshape(-1, 48, 48, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in os.listdir(valid_images_path):\n",
    "    cat_path = os.path.join(valid_images_path, category)\n",
    "    for image in os.listdir(cat_path):\n",
    "        img_path = os.path.join(cat_path, image)\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        img_array = cv2.resize(img, (48,48))\n",
    "        valid_x.append(img_array)\n",
    "        valid_y.append(classes.index(category))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_y = utils.to_categorical(np.array(valid_y), num_classes=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_x = np.array(valid_x).reshape(-1, 48, 48, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization,AveragePooling2D\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms_model = Sequential()\n",
    "\n",
    "ms_model.add(Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu', input_shape=train_flow.image_shape, kernel_regularizer=l2(0.01)))\n",
    "ms_model.add(Conv2D(64, kernel_size=(3,3), padding='same', activation='relu'))\n",
    "ms_model.add(BatchNormalization())\n",
    "ms_model.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\n",
    "ms_model.add(Dropout(0.25))\n",
    "\n",
    "ms_model.add(Conv2D(128, kernel_size=(3,3), padding='same', activation='relu'))\n",
    "ms_model.add(BatchNormalization())\n",
    "ms_model.add(Conv2D(128, kernel_size=(3,3), padding='same', activation='relu'))\n",
    "ms_model.add(BatchNormalization())\n",
    "ms_model.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\n",
    "ms_model.add(Dropout(0.25))\n",
    "\n",
    "ms_model.add(Conv2D(256, kernel_size=(3,3), padding='same', activation='relu'))\n",
    "ms_model.add(BatchNormalization())\n",
    "ms_model.add(Conv2D(256, kernel_size=(3,3), padding='same', activation='relu'))\n",
    "ms_model.add(BatchNormalization())\n",
    "ms_model.add(Conv2D(256, kernel_size=(3,3), padding='same', activation='relu'))\n",
    "ms_model.add(BatchNormalization())\n",
    "ms_model.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\n",
    "ms_model.add(Dropout(0.25))\n",
    "\n",
    "ms_model.add(Conv2D(256, kernel_size=(3,3), padding='same', activation='relu'))\n",
    "ms_model.add(BatchNormalization())\n",
    "ms_model.add(Conv2D(256, kernel_size=(3,3), padding='same', activation='relu'))\n",
    "ms_model.add(BatchNormalization())\n",
    "ms_model.add(Conv2D(256, kernel_size=(3,3), padding='same', activation='relu'))\n",
    "ms_model.add(BatchNormalization())\n",
    "ms_model.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\n",
    "ms_model.add(Dropout(0.25))\n",
    "\n",
    "ms_model.add(Flatten())\n",
    "\n",
    "ms_model.add(Dense(1024, activation='relu'))\n",
    "# ms_model.add(Activation('relu'))\n",
    "ms_model.add(Dropout(0.5))\n",
    "\n",
    "ms_model.add(Dense(1024, activation='relu'))\n",
    "# ms_model.add(Activation('relu'))\n",
    "ms_model.add(Dropout(0.5))\n",
    "\n",
    "# ms_model.add(Flatten())\n",
    "\n",
    "\n",
    "ms_model.add(Dense(train_flow.num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms_model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-7), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms_model.fit(Aug_flow, \n",
    "              epochs=EPOCHS, \n",
    "#               batch_size=BATCH_SIZE,\n",
    "              steps_per_epoch=Aug_flow.n//Aug_flow.batch_size,\n",
    "              validation_data = val_flow,\n",
    "              validation_steps = val_flow.n//val_flow.batch_size,\n",
    "              callbacks=[ReduceLROnPlateau(), EarlyStopping(patience=3)],\n",
    "              shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "448/448 [==============================] - 25s 55ms/step - loss: 0.9777 - accuracy: 0.6430 - val_loss: 1.0560 - val_accuracy: 0.6172\n",
      "Epoch 2/100\n",
      "448/448 [==============================] - 23s 52ms/step - loss: 0.8873 - accuracy: 0.6749 - val_loss: 1.0453 - val_accuracy: 0.6395\n",
      "Epoch 3/100\n",
      "448/448 [==============================] - 23s 51ms/step - loss: 0.8277 - accuracy: 0.6980 - val_loss: 1.0097 - val_accuracy: 0.6350\n",
      "Epoch 4/100\n",
      "448/448 [==============================] - 23s 51ms/step - loss: 0.7919 - accuracy: 0.7140 - val_loss: 1.2202 - val_accuracy: 0.5748\n",
      "Epoch 5/100\n",
      "448/448 [==============================] - 23s 52ms/step - loss: 0.7450 - accuracy: 0.7339 - val_loss: 1.1401 - val_accuracy: 0.6392\n",
      "Epoch 6/100\n",
      "448/448 [==============================] - 23s 52ms/step - loss: 0.6355 - accuracy: 0.7714 - val_loss: 1.1392 - val_accuracy: 0.6411\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1e9acbe8220>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ms_model.fit(x_train, y_train, \n",
    "              epochs=EPOCHS, \n",
    "#               batch_size=BATCH_SIZE,\n",
    "              steps_per_epoch=len(x_train)//BATCH_SIZE,\n",
    "              validation_data = (x_val, y_val),\n",
    "              validation_steps = len(x_val)//BATCH_SIZE,\n",
    "              callbacks=[ReduceLROnPlateau(), EarlyStopping(patience=3)],\n",
    "              shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "897/897 [==============================] - 26s 29ms/step - loss: 0.8407 - accuracy: 0.7065 - val_loss: 1.2384 - val_accuracy: 0.5868\n",
      "Epoch 2/30\n",
      "897/897 [==============================] - 26s 29ms/step - loss: 0.7160 - accuracy: 0.7519 - val_loss: 1.1398 - val_accuracy: 0.6278\n",
      "Epoch 3/30\n",
      "897/897 [==============================] - 26s 29ms/step - loss: 0.6356 - accuracy: 0.7792 - val_loss: 1.1126 - val_accuracy: 0.6353\n",
      "Epoch 4/30\n",
      "897/897 [==============================] - 26s 29ms/step - loss: 0.5846 - accuracy: 0.7996 - val_loss: 1.2628 - val_accuracy: 0.6292\n",
      "Epoch 5/30\n",
      "897/897 [==============================] - 26s 29ms/step - loss: 0.5293 - accuracy: 0.8209 - val_loss: 1.1936 - val_accuracy: 0.6454\n",
      "Epoch 6/30\n",
      "897/897 [==============================] - 26s 29ms/step - loss: 0.4863 - accuracy: 0.8357 - val_loss: 1.2177 - val_accuracy: 0.6501\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1e9acbbaf70>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ms_model.fit(train_flow, \n",
    "              epochs=30, \n",
    "              steps_per_epoch=train_flow.n//train_flow.batch_size, \n",
    "              validation_data = valid_flow, \n",
    "              validation_steps = valid_flow.n//valid_flow.batch_size,\n",
    "             callbacks=[ReduceLROnPlateau(), EarlyStopping(patience=3)]\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms_model_json = ms_model.to_json()\n",
    "with open(\"fer_ms.json\", \"w\") as json_file:\n",
    "    json_file.write(ms_model_json)\n",
    "ms_model.save_weights(\"fer_ms.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow] *",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
